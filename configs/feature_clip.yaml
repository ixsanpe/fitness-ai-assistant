# Feature Pipeline Configuration - CLIP Embeddings

name: "feature_pipeline_clip"

device:
  device: "auto"
  gpu_id: null
  mixed_precision: false

paths:
  project_root: "."
  data_dir: "data"
  raw_data_dir: "data/raw"
  processed_data_dir: "data/processed"
  embeddings_dir: "data/processed/embeddings"
  models_dir: "models"
  results_dir: "results"

embedding:
  embedding_type: "clip"
  model_name: "openai/clip-vit-base-patch32"
  batch_size: 32
  max_length: null
  normalize: true
  output_prefix: "clip"
  save_format: "npy"
  image_processing_mode: "average"
  text_weight: 0.5
  image_weight: 0.5

dataset:
  input_path: "data/raw/exercises"
  output_path: "data/processed/exercises_dataset.jsonl"
  min_text_length: 10
  remove_duplicates: true
