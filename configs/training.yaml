# Training Pipeline Configuration
# This config is used for fine-tuning embedding models

name: "training_pipeline_dev"
version: "0.1.0"
environment: "development"
seed: 42

# Device configuration
device:
  device: "auto"
  gpu_id: null
  mixed_precision: false

# Path configuration
paths:
  project_root: "."
  data_dir: "data"
  raw_data_dir: "data/raw"
  processed_data_dir: "data/processed"
  embeddings_dir: "data/processed/embeddings"
  models_dir: "models"
  results_dir: "results"

# Model to train
model_name: "sentence-transformers/all-MiniLM-L6-v2"
embedding_type: "sentence"

# Training strategy
strategy: "fine_tune"  # Options: supervised, contrastive, triplet, distillation, fine_tune
loss_function: "cosine_similarity"  # Options: cross_entropy, mse, cosine_similarity, triplet_loss, etc.

# Training parameters
num_epochs: 3
max_steps: null  # Overrides num_epochs if set
gradient_accumulation_steps: 1

# Optimizer configuration
optimizer:
  optimizer_type: "adamw"
  learning_rate: 2.0e-5
  weight_decay: 0.01
  betas:
    - 0.9
    - 0.999
  momentum: 0.9
  eps: 1.0e-8
  max_grad_norm: 1.0

# Learning rate scheduler
scheduler:
  scheduler_type: "linear"  # Options: constant, linear, cosine, cosine_with_restarts, etc.
  num_warmup_steps: 0
  num_training_steps: null  # Auto-calculated
  num_cycles: 1
  power: 1.0
  gamma: 0.1
  patience: 10

# Data configuration
data:
  train_path: "data/processed/train.jsonl"
  val_path: null  # Use split from train if null
  test_path: null

  # Data splitting
  val_split: 0.1
  test_split: 0.1

  # Data loading
  batch_size: 32
  eval_batch_size: null  # Same as batch_size if null
  num_workers: 4
  shuffle: true
  drop_last: false

  # Data augmentation
  augmentation_prob: 0.0

# Checkpoint configuration
checkpoint:
  save_dir: "models/checkpoints"
  save_strategy: "epoch"  # Options: epoch, steps, best
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  save_format: "safetensors"  # Options: safetensors, pytorch, onnx

# Evaluation configuration
evaluation:
  eval_strategy: "epoch"  # Options: epoch, steps, no
  eval_steps: 500
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
  metric_for_best_model: "f1"
  greater_is_better: true

# Regularization
dropout: 0.1
label_smoothing: 0.0

# Advanced settings
fp16: false
bf16: false
use_cpu: false
dataloader_pin_memory: true

# Logging and monitoring
logging_steps: 10
log_level: "info"
report_to:
  - "tensorboard"

# Experiment tracking
run_name: null
tags: []
notes: null
